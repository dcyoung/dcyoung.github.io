<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Naive Bayes Classifier in R</title>
<meta name="description" content="Writing a naive bayes classifier from scrach in R.">


  <meta name="author" content="David Young">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="David Young">
<meta property="og:title" content="Naive Bayes Classifier in R">
<meta property="og:url" content="https://dcyoung.github.io/pages/dcyoung/post-naive-bayes-classifiers-in-r/">


  <meta property="og:description" content="Writing a naive bayes classifier from scrach in R.">



  <meta property="og:image" content="https://dcyoung.github.io/pages/dcyoung/images/logos/r.webp">





  <meta property="article:published_time" content="2016-01-01T07:00:00+07:00">



  <meta property="article:modified_time" content="2016-01-01T07:00:00+07:00">




<link rel="canonical" href="https://dcyoung.github.io/pages/dcyoung/post-naive-bayes-classifiers-in-r/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "David Young",
      "url": "https://dcyoung.github.io/pages/dcyoung/",
      "sameAs": ["https://www.linkedin.com/in/david-young-09509210a"]
    
  }
</script>






<!-- end _includes/seo.html -->


  <link href="/pages/dcyoung/feed.xml" type="application/atom+xml" rel="alternate" title="David Young Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript">
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/pages/dcyoung/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->
<link rel="shortcut icon" href="/images/site/favicon.ico">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<meta name="google-site-verification" content="zewSCzlO5JWAuo7MV_u4VoTRLfV4lUrqwxvfo4-3Xfc" />
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: ["tex2jax.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
       processEscapes: true,
       preview: 'none'
     },
     messageStyle: 'none',
     "HTML-CSS": { availableFonts: ["TeX"] }
   });

</script>

<style>
    .bg-color-red {
        background-color: #ffebee;
    }

    .bg-color-green {
        background-color: #e8f5e9;
    }

    .bg-color-yellow {
        background-color: #fff3e0;
    }

    .bg-color-dark-green {
        background-color: #c8e6c9;
    }

    .bg-color-pink {
        background-color: #ffcdd2;
    }

    .bg-color-purple {
        background-color: #c5cae9;
    }

    .pwc-icon {
        width: 23px;
        height: 23px;
        position: relative;
        top: 5px;
        margin-right: 5px;
    }

</style>

  </head>

  <body class="layout--single wide" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/pages/dcyoung/">
          David Young
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/pages/dcyoung/archives/"
                
                
              >Posts</a>
            </li><li class="masthead__menu-item">
              <a
                href="/pages/dcyoung/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/pages/dcyoung/resume/"
                
                
              >Resume</a>
            </li><li class="masthead__menu-item">
              <a
                href="/pages/dcyoung/ml-practitioners-guide/"
                
                
              >ML Practitioner's Guide</a>
            </li><li class="masthead__menu-item">
              <a
                href="/pages/dcyoung/categories/"
                
                
              >Categories</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="https://dcyoung.github.io/pages/dcyoung/">
        <img src="/pages/dcyoung/images/site/profile_artsy.webp" alt="David Young" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://dcyoung.github.io/pages/dcyoung/" itemprop="url">David Young</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>systems thinker &amp; passionate engineer</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://github.com/dcyoung" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/david-young-09509210a" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">Linkedin</span></a></li>
          
        
          
            <li><a href="https://www.instagram.com/cycle_shadez/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i><span class="label">Instagram</span></a></li>
          
        
          
            <li><a href="https://www.youtube.com/channel/UClQEBd-MzkWlkjbxcsve-UA" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i><span class="label">YouTube</span></a></li>
          
        
          
            <li><a href="mailto:david@questionablyartificial.com" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-envelope" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Naive Bayes Classifier in R">
    <meta itemprop="description" content="Writing a naive bayes classifier from scrach in R.">
    <meta itemprop="datePublished" content="2016-01-01T07:00:00+07:00">
    <meta itemprop="dateModified" content="2016-01-01T07:00:00+07:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://dcyoung.github.io/pages/dcyoung/post-naive-bayes-classifiers-in-r/" itemprop="url">Naive Bayes Classifier in R
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#dataset">Dataset</a></li><li><a href="#naive-bayes-classifier">Naive Bayes Classifier</a></li><li><a href="#naive-bayes-classifier-modifications">Naive Bayes Classifier Modifications</a></li><li><a href="#package-based-naive-bayes-classifier">Package Based Naive Bayes Classifier</a></li><li><a href="#comparing-to-svmlight">Comparing to SVMLight</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="dataset">Dataset</h2>

<p>The following code snippets were written to deal with the “Pima Indians” dataset: a famous collection of data on whether a patient has diabetes hosted by the UC Irvine learning data repository. The dataset can be obtained here: <a href="​https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes">​https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes</a></p>

<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>

<p>A simple naive bayes classifier was written from scratch to classify this data set. 20% of the data was used for evaluation, and the other 80% for training. A normal distribution was used to model each of the class-conditional distributions. Over 20 completely separate training/testing trials with a different data split each trial, the average performance of the classifier was 74.67%.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#clear the workspace and console</span><span class="w">
</span><span class="n">rm</span><span class="p">(</span><span class="n">list</span><span class="o">=</span><span class="n">ls</span><span class="p">())</span><span class="w">
</span><span class="n">cat</span><span class="p">(</span><span class="s2">"\014"</span><span class="p">)</span><span class="w"> </span><span class="c1">#code to send ctrl+L to the console and therefore clear the screen</span><span class="w">
</span><span class="c1">#setup working directory </span><span class="w">
</span><span class="n">setwd</span><span class="p">(</span><span class="s1">'~/../DirectoryNameGoesHere'</span><span class="p">)</span><span class="w">
</span><span class="c1">#read all the data into a single table</span><span class="w">
</span><span class="n">allData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'data.txt'</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> 
</span><span class="c1">#import libraries to help with data splitting/partitioning etc.May not be used</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">klaR</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span><span class="w">
 
</span><span class="n">allFeatures</span><span class="o">&lt;-</span><span class="n">allData</span><span class="p">[,</span><span class="o">-</span><span class="nf">c</span><span class="p">(</span><span class="m">9</span><span class="p">)]</span><span class="w"> </span><span class="c1">#features</span><span class="w">
</span><span class="n">labels</span><span class="o">&lt;-</span><span class="n">allData</span><span class="p">[,</span><span class="m">9</span><span class="p">]</span><span class="w"> </span><span class="c1">#labels</span><span class="w">
</span><span class="c1">#partition the data (80% training, 20% testing)</span><span class="w">
</span><span class="n">trainingData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">createDataPartition</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#---------------------------- IGNORE ANY MISSING VALUES -------------------------------------------</span><span class="w">
</span><span class="n">features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">allFeatures</span><span class="w">
 
</span><span class="c1">#grab the subsets of features and labels from the 80% of data used for training</span><span class="w">
</span><span class="n">trainingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="n">trainingData</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">trainingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
 
</span><span class="c1">#partition the training data by class (labeled class)</span><span class="w">
</span><span class="n">trainingPositiveFlag</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainingLabels</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="w">
</span><span class="n">posTrainingExamples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainingFeatures</span><span class="p">[</span><span class="n">trainingPositiveFlag</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">negTrainingExamples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainingFeatures</span><span class="p">[</span><span class="o">!</span><span class="n">trainingPositiveFlag</span><span class="p">,]</span><span class="w">
 
</span><span class="c1">#calcualte the means and standard deviations for both sets (pos and neg) of examples</span><span class="w">
</span><span class="n">posTrainingMeans</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">posTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingMeans</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">negTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">posTrainingStdDev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">posTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingStdDev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">negTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#calculate the likelihoods (class conditional probabilities)</span><span class="w">
</span><span class="c1">#use the means and std dev to calc the logarithmic sum of all the class conditional probabilities</span><span class="w">
</span><span class="n">posTrainingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">trainingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">posTrainingMeans</span><span class="p">)</span><span class="w">
</span><span class="n">posTrainingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">posTrainingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">posTrainingStdDev</span><span class="p">)</span><span class="w">
</span><span class="n">posTrainingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">posTrainingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">posTrainingStdDev</span><span class="p">))</span><span class="w">
 
</span><span class="n">negTrainingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">trainingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">negTrainingMeans</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">negTrainingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">negTrainingStdDev</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">negTrainingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">negTrainingStdDev</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#use the simplified decision rule for a 0-1 decision case</span><span class="w">
</span><span class="n">classifiedTrainingEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">posTrainingLogs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">negTrainingLogs</span><span class="w">
</span><span class="c1">#check against known labels to see where the classifications were accurate</span><span class="w">
</span><span class="n">correctClassifiedTrainEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">classifiedTrainingEgs</span><span class="o">==</span><span class="n">trainingLabels</span><span class="w">
</span><span class="c1">#calculate the classification accuracy for this fold, and store it</span><span class="w">
</span><span class="n">train_accuracy_IGNORE_NA</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTrainEgs</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTrainEgs</span><span class="p">)</span><span class="o">+</span><span class="nf">sum</span><span class="p">(</span><span class="o">!</span><span class="n">correctClassifiedTrainEgs</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#----------Classify the testing data using the means and std deviations from the training data-----------</span><span class="w">
</span><span class="c1">#use the other 20% of data (untouched) as a testing set</span><span class="w">
</span><span class="n">testingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">testingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
 
</span><span class="c1">#calculate the likelihoods (class conditional probabilities)</span><span class="w">
</span><span class="c1"># but this time, use the means and std dev FROM THE TRAINING DATA </span><span class="w">
</span><span class="c1"># to calc the logarithmic sum of all the class conditional probabilities</span><span class="w">
</span><span class="n">posTestingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">testingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">posTrainingMeans</span><span class="p">)</span><span class="w">  </span><span class="c1">#use the training data means</span><span class="w">
</span><span class="n">posTestingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">posTestingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">posTrainingStdDev</span><span class="p">)</span><span class="w"> </span><span class="c1">#use the training data std dev</span><span class="w">
</span><span class="n">posTestingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">posTestingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">posTrainingStdDev</span><span class="p">))</span><span class="w"> </span><span class="c1">#use the training data std dev</span><span class="w">
</span><span class="n">negTestingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">testingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">negTrainingMeans</span><span class="p">)</span><span class="w">
</span><span class="n">negTestingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">negTestingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">negTrainingStdDev</span><span class="p">)</span><span class="w">
</span><span class="n">negTestingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">negTestingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">negTrainingStdDev</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#use the simplified decision rule for a 0-1 decision case</span><span class="w">
</span><span class="n">classifiedTestingEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">posTestingLogs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">negTestingLogs</span><span class="w">
</span><span class="c1">#check against known labels to see where the classifications were accurate</span><span class="w">
</span><span class="n">correctClassifiedTestEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">classifiedTestingEgs</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">testingLabels</span><span class="w">
</span><span class="c1">#calculate the classification accuracy for this fold, and store it</span><span class="w">
</span><span class="n">test_accuracy_IGNORE_NA</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTestEgs</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTestEgs</span><span class="p">)</span><span class="o">+</span><span class="nf">sum</span><span class="p">(</span><span class="o">!</span><span class="n">correctClassifiedTestEgs</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h2 id="naive-bayes-classifier-modifications">Naive Bayes Classifier Modifications</h2>

<p>After running the previous classifier, the code was adjusted to account for a few of the patient attributes containing missing values for some examples.  Using the exact same 20 data splits from the unmodified bayes classifier above, the version that acknowledged missing values had an average classification accuracy of 73.89%. Recall the accuracy of the unmodified version which ignored missing values was 74.67%. Surprisingly, for most trials, the classification accuracy actually dropped when considering the missing values. I would have expected an improvement. The drop wasn’t large (~1-2%) but it was noticeable. One possible explanation for this decrease is that the naive bayes model was implemented using a normal distribution to model the features, which is a very inaccurate assumption. When including missing values as 0’s (1st classifier) the normal distribution is skewed to the left inaccurately. When removing the missing values from features with many missing examples (modification), the mean shifts to the right. Because the class-conditional probability distributions were heavily skewed, the unmodified classifier might have yielded a higher accuracy.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#----------------------------NOW ACKNOWLEDGE THE MISSING VALUES ------------------------------------</span><span class="w">
 
</span><span class="c1">#adjust the features such that a 0 is reported as "NA"</span><span class="w">
</span><span class="c1">#for each feature in question (3,4,6 and 8)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">f</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">))</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="c1">#determine which examples had a 0 for this feature (ie: unknown value)</span><span class="w">
  </span><span class="n">examplesMissingFeatureF</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">allFeatures</span><span class="p">[,</span><span class="w"> </span><span class="n">f</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="w">
  </span><span class="c1">#replace all these missing values with an NA</span><span class="w">
  </span><span class="n">features</span><span class="p">[</span><span class="n">examplesMissingFeatureF</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA</span><span class="w">
</span><span class="p">}</span><span class="w">
 
</span><span class="c1">#grab the subsets of features and labels from the 80% of data used for training</span><span class="w">
</span><span class="n">trainingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="n">trainingData</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">trainingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
 
</span><span class="c1">#partition the training data by class (labeled class)</span><span class="w">
</span><span class="n">trainingPositiveFlag</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainingLabels</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="w">
</span><span class="n">posTrainingExamples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainingFeatures</span><span class="p">[</span><span class="n">trainingPositiveFlag</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">negTrainingExamples</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">trainingFeatures</span><span class="p">[</span><span class="o">!</span><span class="n">trainingPositiveFlag</span><span class="p">,]</span><span class="w">
 
</span><span class="c1">#calcualte the means and standard deviations for both sets (pos and neg) of examples</span><span class="w">
</span><span class="n">posTrainingMeans</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">posTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingMeans</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">negTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">posTrainingStdDev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">posTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingStdDev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">negTrainingExamples</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#calculate the likelihoods (class conditional probabilities)</span><span class="w">
</span><span class="c1">#use the means and std dev to calc the logarithmic sum of all the class conditional probabilities</span><span class="w">
</span><span class="n">posTrainingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">trainingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">posTrainingMeans</span><span class="p">)</span><span class="w">
</span><span class="n">posTrainingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">posTrainingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">posTrainingStdDev</span><span class="p">)</span><span class="w">
</span><span class="n">posTrainingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">posTrainingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">posTrainingStdDev</span><span class="p">))</span><span class="w">
 
</span><span class="n">negTrainingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">trainingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">negTrainingMeans</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">negTrainingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">negTrainingStdDev</span><span class="p">)</span><span class="w">
</span><span class="n">negTrainingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">negTrainingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">negTrainingStdDev</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#use the simplified decision rule for a 0-1 decision case</span><span class="w">
</span><span class="n">classifiedTrainingEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">posTrainingLogs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">negTrainingLogs</span><span class="w">
</span><span class="c1">#check against known labels to see where the classifications were accurate</span><span class="w">
</span><span class="n">correctClassifiedTrainEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">classifiedTrainingEgs</span><span class="o">==</span><span class="n">trainingLabels</span><span class="w">
</span><span class="c1">#calculate the classification accuracy for this fold, and store it</span><span class="w">
</span><span class="n">train_accuracy_ACKNOWLEDGE_NA</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTrainEgs</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTrainEgs</span><span class="p">)</span><span class="o">+</span><span class="nf">sum</span><span class="p">(</span><span class="o">!</span><span class="n">correctClassifiedTrainEgs</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#----------Classify the testing data using the means and std deviations from the training data-----------</span><span class="w">
</span><span class="c1">#use the other 20% of data (untouched) as a testing set</span><span class="w">
</span><span class="n">testingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">testingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
 
</span><span class="c1">#calculate the likelihoods (class conditional probabilities)</span><span class="w">
</span><span class="c1"># but this time, use the means and std dev FROM THE TRAINING DATA </span><span class="w">
</span><span class="c1"># to calc the logarithmic sum of all the class conditional probabilities</span><span class="w">
</span><span class="n">posTestingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">testingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">posTrainingMeans</span><span class="p">)</span><span class="w">  </span><span class="c1">#use the training data means</span><span class="w">
</span><span class="n">posTestingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">posTestingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">posTrainingStdDev</span><span class="p">)</span><span class="w"> </span><span class="c1">#use the training data std dev</span><span class="w">
</span><span class="n">posTestingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">posTestingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">posTrainingStdDev</span><span class="p">))</span><span class="w"> </span><span class="c1">#use the training data std dev</span><span class="w">
</span><span class="n">negTestingOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">testingFeatures</span><span class="p">)</span><span class="o">-</span><span class="n">negTrainingMeans</span><span class="p">)</span><span class="w">
</span><span class="n">negTestingScaledOffsets</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">negTestingOffsets</span><span class="p">)</span><span class="o">/</span><span class="n">negTrainingStdDev</span><span class="p">)</span><span class="w">
</span><span class="n">negTestingLogs</span><span class="w"> </span><span class="o">&lt;--</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="n">rowSums</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">negTestingScaledOffsets</span><span class="p">,</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">negTrainingStdDev</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#use the simplified decision rule for a 0-1 decision case</span><span class="w">
</span><span class="n">classifiedTestingEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">posTestingLogs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">negTestingLogs</span><span class="w">
</span><span class="c1">#check against known labels to see where the classifications were accurate</span><span class="w">
</span><span class="n">correctClassifiedTestEgs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">classifiedTestingEgs</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">testingLabels</span><span class="w">
</span><span class="c1">#calculate the classification accuracy for this fold, and store it</span><span class="w">
</span><span class="n">test_accuracy_ACKNOWLEDGE_NA</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTestEgs</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifiedTestEgs</span><span class="p">)</span><span class="o">+</span><span class="nf">sum</span><span class="p">(</span><span class="o">!</span><span class="n">correctClassifiedTestEgs</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h2 id="package-based-naive-bayes-classifier">Package Based Naive Bayes Classifier</h2>

<p>Now, a naive bayes classifier was constructed using the klaR and caret packages. The caret package was used for cross-validation while the klaR package was used to estimate the class-conditional densities using a density estimation procedure. This classifier assumes no attribute has a missing value.</p>

<p>The average over 20 different data split trials was 75.85%. So, allowing a more optimized package to construct the naive bayes model yielded slightly improved performance. The performance gains here can be attributed to the models used for the features. The packages are free to use models other than a normal distribution which is likely more accurate for some features.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#read all the data into a single table</span><span class="w">
</span><span class="n">allData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'data.txt'</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> 
 
</span><span class="c1">#import libraries to help with data splitting/partitioning, </span><span class="w">
</span><span class="c1">#cross validation and easy classifier construction</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">klaR</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#grab the features from the main data file, removing the labels </span><span class="w">
</span><span class="c1">#assume no data is missing... ie: ignore missing values without noting them as NA</span><span class="w">
</span><span class="n">features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">allData</span><span class="p">[,</span><span class="o">-</span><span class="nf">c</span><span class="p">(</span><span class="m">9</span><span class="p">)]</span><span class="w">
 
</span><span class="c1">#grab the labels from the main data file... use as.factor to make </span><span class="w">
</span><span class="c1">#the format comptabile with future functions to be used</span><span class="w">
</span><span class="n">labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">allData</span><span class="p">[,</span><span class="m">9</span><span class="p">])</span><span class="w">
 
</span><span class="c1">#split the data into 80% training data and 20% testing data</span><span class="w">
</span><span class="n">trainingData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">createDataPartition</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
 
</span><span class="n">trainingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="n">trainingData</span><span class="p">,]</span><span class="w">
</span><span class="n">trainingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
</span><span class="n">testingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">,]</span><span class="w">
</span><span class="n">testingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
 
</span><span class="c1">#Fit Predictive Models over Different Tuning Parameters</span><span class="w">
</span><span class="c1">#in this case...</span><span class="w">
</span><span class="c1">#   use a naive bayes model for the classifier ('nb')</span><span class="w">
</span><span class="c1">#   use cross validation resampling method with 10 folds/resampling iterations ('cv')</span><span class="w">
</span><span class="n">classifierModel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="n">trainingFeatures</span><span class="p">,</span><span class="w"> </span><span class="n">trainingLabels</span><span class="p">,</span><span class="w"> </span><span class="s1">'nb'</span><span class="p">,</span><span class="w"> </span><span class="n">trControl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">trainControl</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'cv'</span><span class="p">,</span><span class="w"> </span><span class="n">number</span><span class="o">=</span><span class="m">10</span><span class="p">))</span><span class="w">
 
</span><span class="c1">#use the trained model to predict class labels for the testing data</span><span class="w">
</span><span class="c1">#note: argument newdata specifies the first place to look for explanatory variables to be used for prediction</span><span class="w">
</span><span class="n">predictedLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">classifierModel</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">testingFeatures</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#compare the predicted labels to the actual known labels for the testing data</span><span class="w">
</span><span class="n">confusionMatrix</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">predictedLabels</span><span class="p">,</span><span class="w"> </span><span class="n">testingLabels</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h2 id="comparing-to-svmlight">Comparing to SVMLight</h2>

<p>SVMLight and klaR were used to train an SVM (support vector machine) to classify the same data. 20% of the data was held out for testing while the other 80% was used for training.</p>

<p>The average classification accuracy over 20 separate data split trials was 76.34%. Again there was a small performance improvement, even over the more optimized naive bayes classifier. The SVM classifier abstracts the parameters that model specific distributions into weights and values that are free to adjust to the training data more than the statistical models can permit. Provided enough training data, this yields very good results.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#read all the data into a single table</span><span class="w">
</span><span class="n">allData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'data.txt'</span><span class="p">,</span><span class="w"> </span><span class="n">header</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> 
 
</span><span class="c1">#import libraries to help with data splitting/partitioning, </span><span class="w">
</span><span class="c1">#cross validation etc.</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">klaR</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#grab the features from the main data file, removing the labels </span><span class="w">
</span><span class="c1">#assume no data is missing... ie: ignore missing values without noting them as NA</span><span class="w">
</span><span class="n">features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">allData</span><span class="p">[,</span><span class="o">-</span><span class="nf">c</span><span class="p">(</span><span class="m">9</span><span class="p">)]</span><span class="w">
 
</span><span class="c1">#grab the labels from the main data file... use as.factor to make </span><span class="w">
</span><span class="c1">#the format comptabile with future functions to be used</span><span class="w">
</span><span class="n">labels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">allData</span><span class="p">[,</span><span class="m">9</span><span class="p">])</span><span class="w">
 
</span><span class="c1">#split the data into 80% training data and 20% testing data</span><span class="w">
</span><span class="n">trainingData</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">createDataPartition</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
 
</span><span class="n">trainingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="n">trainingData</span><span class="p">,]</span><span class="w">
</span><span class="n">trainingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
</span><span class="n">testingFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">,]</span><span class="w">
</span><span class="n">testingLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="n">trainingData</span><span class="p">]</span><span class="w">
 
</span><span class="c1">#train an svm using the training data </span><span class="w">
</span><span class="n">svm</span><span class="o">&lt;-</span><span class="n">svmlight</span><span class="p">(</span><span class="n">trainingFeatures</span><span class="p">,</span><span class="w"> </span><span class="n">trainingLabels</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#use the trained svm model to predict classes for the testing features</span><span class="w">
</span><span class="n">predictedLabels</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span><span class="w"> </span><span class="n">testingFeatures</span><span class="p">)</span><span class="w">
 
</span><span class="c1">#determine where the classifications were correct</span><span class="w">
</span><span class="n">correctClassifications</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predictedLabels</span><span class="o">$</span><span class="n">class</span><span class="w">
 
</span><span class="c1">#calculate the accuracy of the classifier</span><span class="w">
</span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifications</span><span class="o">==</span><span class="n">testingLabels</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">correctClassifications</span><span class="o">==</span><span class="n">testingLabels</span><span class="p">)</span><span class="o">+</span><span class="nf">sum</span><span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">correctClassifications</span><span class="o">==</span><span class="n">testingLabels</span><span class="p">)))</span><span class="w">
</span></code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/pages/dcyoung/categories/#ai" class="page__taxonomy-item p-category" rel="tag">ai</a><span class="sep">, </span>
    
      <a href="/pages/dcyoung/categories/#machine-learning" class="page__taxonomy-item p-category" rel="tag">machine learning</a><span class="sep">, </span>
    
      <a href="/pages/dcyoung/categories/#school-project" class="page__taxonomy-item p-category" rel="tag">school project</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2016-01-01">January 1, 2016</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=Naive+Bayes+Classifier+in+R%20https%3A%2F%2Fdcyoung.github.io%2Fpages%2Fdcyoung%2Fpost-naive-bayes-classifiers-in-r%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdcyoung.github.io%2Fpages%2Fdcyoung%2Fpost-naive-bayes-classifiers-in-r%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://dcyoung.github.io/pages/dcyoung/post-naive-bayes-classifiers-in-r/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/pages/dcyoung/post-experimenting-with-tracking-and-depth-perception-in-vr/" class="pagination--pager" title="Experimenting with Tracking &amp; Depth Perception in VR
">Previous</a>
    
    
      <a href="/pages/dcyoung/post-laplacian-blob-detector/" class="pagination--pager" title="Laplacian Blob Detector
">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Comments</h4>
      <section id="utterances-comments"></section>
    
</div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You May Also Enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/pages/dcyoung/images/r3f-nn-visualizer/preview.webp" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pages/dcyoung/post-r3f-nn-visualizer/" rel="permalink">Interactive Neural Network Visualizer
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">An interactive Neural Network visualization built w/ modern web technologies including tensorflow.js and react-three-fiber.
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/pages/dcyoung/images/mmle-scores/softmax.webp" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pages/dcyoung/post-mmle-scores/" rel="permalink">Practical ML: Detecting Out-of-Distribution Data
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Detecting out of distribution samples using
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/pages/dcyoung/images/gh-pages-staging-deployments/preview.webp" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pages/dcyoung/post-gh-pages-staging-deployments/" rel="permalink">Automating Free Staging Deployments for Github Pages
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Automating free staging deployments for Github Pages using Github Actions.
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/pages/dcyoung/images/clustering-custom-distance/haversine.webp" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/pages/dcyoung/post-clustering-custom-distance/" rel="permalink">Performant Clustering of Geo Coordinates w/ Custom Distance Functions
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Implementing vectorized clustering methods for distance metrics unsupported by common libraries.
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://github.com/dcyoung" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.instagram.com/cycle_shadez/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/pages/dcyoung/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 <a href="https://dcyoung.github.io">David Young</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/pages/dcyoung/assets/js/main.min.js"></script>




<script src="/pages/dcyoung/assets/js/lunr/lunr.min.js"></script>
<script src="/pages/dcyoung/assets/js/lunr/lunr-store.js"></script>
<script src="/pages/dcyoung/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8LFRSKS1E8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-8LFRSKS1E8', { 'anonymize_ip': false});
</script>






    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'dcyoung/dcyoung.github.io');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('label', 'comment');
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  




  </body>
</html>
